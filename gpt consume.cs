As of today, specific information about how much power ChatGPT consumes is not publicly available in precise terms. However, running large-scale AI models like ChatGPT requires significant computational resources and energy. These AI systems are typically hosted on high-performance cloud infrastructure (like those provided by data centers such as those run by Microsoft Azure or Google Cloud), which can consume a large amount of electricity for both computation and cooling.

Here are some factors that affect the energy consumption of a system like ChatGPT:

1. **Model Size and Complexity**: GPT-4 (the architecture underlying ChatGPT) is a large neural network with billions of parameters, which requires substantial computational power to process requests, especially during inference (i.e., when responding to user queries).

2. **Hardware**: ChatGPT models are typically run on powerful hardware, such as GPUs (Graphics Processing Units) or TPUs (Tensor Processing Units). These can consume between 200-400 watts per unit during peak load, and large-scale deployments can involve thousands of such units running simultaneously.

3. **Data Center Infrastructure**: Data centers hosting AI models consume significant amounts of power for cooling and maintaining optimal working conditions. The energy usage of these facilities is another important aspect of overall consumption.

4. **User Load**: The power consumption also scales with the number of requests (queries) processed. During high-traffic periods, more computational resources are required, which increases power consumption.

### Estimates
There have been some studies and estimates on the energy consumption of AI models. For example:
- Training large language models like GPT-3 (a predecessor to GPT-4) was estimated to consume hundreds of megawatt-hours of electricity.
- Inference, or running the model in production (like ChatGPT does), typically consumes less energy than training, but still significant amounts due to the scale of user interactions.

### Sustainability Initiatives
Major cloud providers like Microsoft (partnered with OpenAI), Google, and Amazon have been investing heavily in renewable energy to power their data centers and reduce the carbon footprint of AI services. Many large data centers now use solar, wind, or other green energy sources to offset energy consumption.

To summarize, while precise power consumption figures for ChatGPT today are not publicly available, running a model of this scale is energy-intensive, but efforts are being made to make it more sustainable.